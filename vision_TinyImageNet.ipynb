{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File for running all computer vision experiments.\n",
    "\"\"\"\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "from re import sub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, LBFGS, SGD\n",
    "import tqdm\n",
    "\n",
    "# Our new algorithm\n",
    "from online_conformal.magnitude_learner import MagnitudeLearner, MagnitudeLearnerV2\n",
    "from online_conformal.mag_learner_undiscounted import MagLearnUndiscounted\n",
    "from online_conformal.ogd_simple import SimpleOGD\n",
    "\n",
    "# From previous work\n",
    "from online_conformal.saocp import SAOCP\n",
    "from online_conformal.faci import FACI, FACI_S\n",
    "from online_conformal.nex_conformal import NExConformal\n",
    "from online_conformal.ogd import ScaleFreeOGD\n",
    "from online_conformal.split_conformal import SplitConformal\n",
    "from online_conformal.utils import pinball_loss\n",
    "from cv_utils import create_model, data_loader\n",
    "from cv_utils import ImageNet, TinyImageNet, CIFAR10, CIFAR100, ImageNetC, TinyImageNetC, CIFAR10C, CIFAR100C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"vision.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corruptions = [\n",
    "    None,\n",
    "    \"brightness\",\n",
    "    \"contrast\",\n",
    "    \"defocus_blur\",\n",
    "    \"elastic_transform\",\n",
    "    \"fog\",\n",
    "    \"frost\",\n",
    "    \"gaussian_noise\",\n",
    "    \"glass_blur\",\n",
    "    \"impulse_noise\",\n",
    "    \"jpeg_compression\",\n",
    "    \"motion_blur\",\n",
    "    \"pixelate\",\n",
    "    \"shot_noise\",\n",
    "    \"snow\",\n",
    "    \"zoom_blur\",\n",
    "]\n",
    "\n",
    "def parse_args():\n",
    "    args = argparse.Namespace\n",
    "    args.dataset = \"TinyImageNet\"\n",
    "    args.model = \"resnet50\"\n",
    "    args.lr = 1e-3\n",
    "    args.batch_size = 256\n",
    "    args.n_epochs = 150 #default = 150\n",
    "    args.patience = 10\n",
    "    #args.ignore_checkpoint = \"store_true\"\n",
    "    args.target_cov = 90\n",
    "    args.ignore_checkpoint = True\n",
    "\n",
    "    assert 50 < args.target_cov < 100\n",
    "    args.target_cov = args.target_cov / 100\n",
    "\n",
    "    # Set up distributed training if desired, and set the device\n",
    "    args.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    print(\"args_local_rank = \", args.local_rank)\n",
    "    if args.local_rank == -1:\n",
    "        if torch.cuda.is_available():\n",
    "            args.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            args.device = torch.device(\"cpu\")\n",
    "        args.world_size = 1\n",
    "    else:\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "        args.device = torch.device(args.local_rank)\n",
    "        args.world_size = dist.get_world_size()\n",
    "    print(\"args.devices = \", args.device)\n",
    "        \n",
    "    return args\n",
    "\n",
    "parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_dataset(dataset, split):\n",
    "    if dataset == \"ImageNet\":\n",
    "        return ImageNet(split)\n",
    "    elif dataset == \"TinyImageNet\":\n",
    "        return TinyImageNet(split)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        return CIFAR10(split)\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        return CIFAR100(split)\n",
    "    raise ValueError(f\"Dataset {dataset} is not supported.\")\n",
    "\n",
    "\n",
    "def get_model_file(args):\n",
    "    rootdir = os.path.dirname(os.path.abspath(__file__))\n",
    "    return os.path.join(rootdir, \"cv_models\", args.dataset, args.model, \"model.pt\")\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    if args.dataset != \"ImageNet\":\n",
    "        return torch.load(get_model_file(args), map_location=args.device)\n",
    "    return create_model(dataset=ImageNet(\"valid\"), model_name=args.model, device=args.device)\n",
    "\n",
    "\n",
    "def get_results_file(args, corruption, severity):\n",
    "    rootdir = os.path.dirname(os.path.abspath(__file__))\n",
    "    return os.path.join(rootdir, \"cv_logits\", args.dataset, args.model, f\"{corruption}_{severity}.pt\")\n",
    "\n",
    "\n",
    "def get_temp_file(args):\n",
    "    return os.path.join(os.path.dirname(get_results_file(args, None, 0)), \"temp.txt\")\n",
    "\n",
    "\n",
    "def finished(args):\n",
    "    for corruption in corruptions:\n",
    "        for severity in [0] if corruption is None else [1, 2, 3, 4, 5]:\n",
    "            fname = get_results_file(args, corruption, severity)\n",
    "            if not os.path.isfile(fname):\n",
    "                return False\n",
    "    return os.path.isfile(get_temp_file(args))\n",
    "\n",
    "\n",
    "def raps_params(dataset):\n",
    "    if dataset == \"CIFAR10\":\n",
    "        lmbda, k_reg, n_class = 0.1, 1, 10\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        lmbda, k_reg, n_class = 0.02, 5, 100\n",
    "    elif dataset == \"TinyImageNet\":\n",
    "        lmbda, k_reg, n_class = 0.01, 20, 200\n",
    "    elif dataset == \"ImageNet\":\n",
    "        lmbda, k_reg, n_class = 0.01, 10, 1000\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset {dataset}\")\n",
    "    return lmbda, k_reg, n_class\n",
    "\n",
    "\n",
    "def temperature_scaling(args):\n",
    "    temp = nn.Parameter(torch.tensor(1.0, device=args.device))\n",
    "    opt = LBFGS([temp], lr=0.01, max_iter=500)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_epochs = 10\n",
    "    valid_data = get_base_dataset(args.dataset, \"valid\")\n",
    "    model = get_model(args)\n",
    "    for epoch in range(n_epochs):\n",
    "        valid_loader = data_loader(valid_data, batch_size=args.batch_size, epoch=epoch)\n",
    "        for x, y in tqdm.tqdm(valid_loader, desc=f\"Calibration epoch {epoch + 1:2}/{n_epochs}\", disable=False):\n",
    "            with torch.no_grad():\n",
    "                logits = model(x.to(device=args.device))\n",
    "\n",
    "            def eval():\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(logits / temp, y.to(device=args.device))\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            opt.step(eval)\n",
    "\n",
    "    return temp.item()\n",
    "\n",
    "\n",
    "def get_logits(args):\n",
    "    if args.dataset == \"CIFAR10\":\n",
    "        dataset_cls = CIFAR10C\n",
    "    elif args.dataset == \"CIFAR100\":\n",
    "        dataset_cls = CIFAR100C\n",
    "    elif args.dataset == \"TinyImageNet\":\n",
    "        dataset_cls = TinyImageNetC\n",
    "    elif args.dataset == \"ImageNet\":\n",
    "        dataset_cls = ImageNetC\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {args.dataset} is not supported.\")\n",
    "    model = None\n",
    "    print(\"Dataset: \", dataset_cls)\n",
    "\n",
    "    print(\"Applying corruptions: \", corruptions)\n",
    "    for corruption in tqdm.tqdm(corruptions, desc=\"Corruptions\", position=1):\n",
    "        print(\"Corruption: \", corruption)\n",
    "        severities = [0] if corruption is None else [1, 2, 3, 4, 5]\n",
    "        print(\"Applying various severity levels: \", severities)\n",
    "        for severity in tqdm.tqdm(severities, desc=\"Severity Levels\", position=2, leave=False):\n",
    "            print(\"Severity: \", severity)\n",
    "            fname = get_results_file(args, corruption, severity)\n",
    "            if os.path.isfile(fname) and not args.ignore_checkpoint:\n",
    "                continue\n",
    "            os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "            if model is None:\n",
    "                model = get_model(args)\n",
    "\n",
    "            # Save the model's logits & labels for the whole dataset\n",
    "            print(\"Save the model's logits & labels for the whole dataset\")\n",
    "            logits, labels = [], []\n",
    "            dataset = dataset_cls(corruption=corruption, severity=severity)\n",
    "            loader = data_loader(dataset, batch_size=args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    logits.append(model(x.to(device=args.device)).cpu())\n",
    "                    labels.append(y.cpu())\n",
    "            torch.save([torch.cat(logits), torch.cat(labels)], fname)\n",
    "\n",
    "\n",
    "def t_to_sev(t, window, run_length=500, schedule=None):\n",
    "    if t < window or schedule in [None, \"None\", \"none\"]:\n",
    "        return 0\n",
    "    t_base = t - window // 2\n",
    "    if schedule == \"gradual\":\n",
    "        k = (t_base // run_length) % 10\n",
    "        return k if k <= 5 else 10 - k\n",
    "    if schedule == \"random_sudden\":\n",
    "        return np.clip(np.random.randint(0, 10) * ((t_base // run_length) % 2),0,5)\n",
    "    if schedule == \"random_gradual\":\n",
    "        k = (((t_base* abs(np.random.uniform(1,1.5))) // run_length) % 10 ) \n",
    "        return (k if k <= 5 else 10 - k) * np.random.randint(1,2) \n",
    "    return 5 * ((t_base // run_length) % 2) # default: sudden schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = parse_args()\n",
    "if args.local_rank in [-1, 0]:\n",
    "    print(\"Getting temp file...\")\n",
    "    temp_file = get_temp_file(args)\n",
    "    print(\"Done\")\n",
    "    if not finished(args):\n",
    "        print(\"get_logits(args)\")\n",
    "        get_logits(args)\n",
    "        print(\"...Done\")\n",
    "        temp = temperature_scaling(args)\n",
    "        with open(temp_file, \"w\") as f:\n",
    "            f.write(str(temp))\n",
    "\n",
    "    # Load the saved logits\n",
    "    print(\"Load the saved logits\")\n",
    "    with open(temp_file) as f:\n",
    "        temp = float(f.readline())\n",
    "    n_data = None\n",
    "    sev2results = defaultdict(list)\n",
    "    for corruption in corruptions:\n",
    "        severities = [0] if corruption is None else [1, 2, 3, 4, 5]\n",
    "        for severity in severities:\n",
    "            try:\n",
    "                logits, labels = torch.load(get_results_file(args, corruption, severity))\n",
    "            except:\n",
    "                continue\n",
    "            sev2results[severity].append(list(zip(F.softmax(logits / temp, dim=-1).numpy(), labels.numpy())))\n",
    "            n_data = len(labels) if n_data is None else min(n_data, len(labels))\n",
    "\n",
    "    # Initialize conformal prediction methods, along with accumulators for results\n",
    "    print(\"Initialize conformal prediction methods, along with accumulators for results\")\n",
    "    lmbda, k_reg, n_class = raps_params(args.dataset)\n",
    "    D_old = 1 + lmbda * np.sqrt(n_class - k_reg)\n",
    "    D = D_old\n",
    "    methods = [MagnitudeLearner, MagLearnUndiscounted, MagnitudeLearnerV2]\n",
    "    label2err = defaultdict(list)\n",
    "    plt.rcParams[\"text.usetex\"] = True\n",
    "    h = 5 + 0.5 * (len(methods) > 5)\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, sharex=\"col\", sharey=\"row\", figsize=(12, h), height_ratios=[4, 4, 2])\n",
    "    np.random.seed(0)\n",
    "    for i_shift, shift in enumerate([\"sudden\", \"gradual\"]):\n",
    "        sevs, s_opts, w_opts = [], [], []\n",
    "        # warmup, window, run_length = 1000, 100, 500 # original code\n",
    "        warmup, window, run_length = 1000, 25, 1000 # our code\n",
    "        state = np.random.RandomState(0)\n",
    "        order = state.permutation(n_data)[: 6 * run_length + window // 2 + warmup]\n",
    "        coverages, s_hats, widths = [{m.__name__: [] for m in methods} for _ in range(3)]\n",
    "        predictors = [m(None, None, max_scale = D, lifetime = 32, coverage = args.target_cov) for m in methods]\n",
    "        t_vec = np.zeros(len(order))\n",
    "        for t, i in tqdm.tqdm(enumerate(order, start=-warmup), total=len(order)):\n",
    "            sev = t_to_sev(t, window=window, schedule = shift)\n",
    "            probs, label = sev2results[sev][state.randint(0, len(sev2results[sev]))][i]\n",
    "\n",
    "            # Convert probability to APS score\n",
    "            i_sort = np.flip(np.argsort(probs))\n",
    "            p_sort_cumsum = np.cumsum(probs[i_sort]) - state.rand() * probs[i_sort]\n",
    "            s_sort_cumsum = p_sort_cumsum + lmbda * np.sqrt(np.cumsum([i > k_reg for i in range(n_class)]))\n",
    "            w_opt = np.argsort(i_sort)[label] + 1\n",
    "            s_opt = s_sort_cumsum[w_opt - 1]\n",
    "            if t >= 0:\n",
    "                sevs.append(sev)\n",
    "                s_opts.append(s_opt)\n",
    "                w_opts.append(w_opt)\n",
    "                t_vec[t] = t\n",
    "\n",
    "            # Update all the conformal predictors\n",
    "            for predictor in predictors:\n",
    "                name = type(predictor).__name__\n",
    "                if t >= 0:\n",
    "                    _, s_hat = predictor.predict(horizon=1)\n",
    "                    w = np.sum(s_sort_cumsum <= s_hat)\n",
    "                    s_hats[name].append(s_hat)\n",
    "                    widths[name].append(w)\n",
    "                    coverages[name].append(w >= w_opt)\n",
    "                predictor.update(ground_truth=pd.Series([s_opt]), forecast=pd.Series([0]), horizon=1)\n",
    "\n",
    "        # Perform evaluation & produce a pretty graph\n",
    "        plot_loss = False\n",
    "        for ax in axs[:, i_shift]:\n",
    "            ax.xaxis.grid(False)\n",
    "            ax.tick_params(axis=\"both\", which=\"both\", labelsize=10)\n",
    "\n",
    "        ax1, ax2, ax3 = axs[:, i_shift]\n",
    "        sevs = pd.Series(sevs).rolling(window).mean().dropna()\n",
    "        w_opts = pd.Series(s_opts if plot_loss else w_opts).rolling(window).quantile(args.target_cov).dropna()\n",
    "        ax1.set_ylabel(\"Local Coverage\", fontsize=10)\n",
    "        ax2.set_ylabel(\"Prediction Set Size\", fontsize=10)\n",
    "        ax3.set_xlabel(\"Time\", fontsize=10)\n",
    "        ax3.set_ylabel(\"Corruption Level\", fontsize=10)\n",
    "        ax1.axhline(args.target_cov, c=\"k\", ls=\"--\", lw=2, zorder=len(methods), label=\"Best Fixed\")\n",
    "        ax2.plot(np.arange(len(w_opts)), gaussian_filter1d(w_opts, sigma=2), c=\"k\", ls=\"--\", lw=2, zorder=len(methods))\n",
    "        ax3.plot(np.arange(len(sevs)), sevs, c=\"k\")\n",
    "\n",
    "        s_opts = np.asarray(s_opts)\n",
    "        int_q = pd.Series(s_opts).rolling(window).quantile(args.target_cov).dropna()\n",
    "        print(f\"Distribution shift: {shift}\")\n",
    "        for i, m in enumerate(methods):\n",
    "            # Compute various summary statistics\n",
    "            name = m.__name__ # name of the method (OGD, SAOCP, etc.)\n",
    "            label = sub(\"Split\", \"S\", sub(\"Conformal\", \"CP\", sub(\"ScaleFree\", \"SF-\", sub(\"_\", \"-\", name))))\n",
    "            if name == \"MagnitudeLearner\":\n",
    "                label = \"Mag. Learner ($\\lambda_t < 1$)\"\n",
    "            if name == \"MagLearnUndiscounted\":\n",
    "                label = \"Mag. Learner ($\\lambda_t = 1$)\"\n",
    "            if name == \"MagnitudeLearnerV2\":\n",
    "                label = \"Mag. Learner ($\\lambda_t < 1, h_t = 0$)\"\n",
    "            s_hat = np.asarray(s_hats[name])\n",
    "            int_cov = gaussian_filter1d(pd.Series(coverages[name]).rolling(window).mean().dropna(), sigma=3)\n",
    "            int_w = pd.Series(s_hats[name] if plot_loss else widths[name]).rolling(window).mean().dropna()\n",
    "            int_losses = pd.Series(pinball_loss(s_opts, s_hat, args.target_cov)).rolling(window).mean().dropna()\n",
    "            opts = [pinball_loss(s_opts[i : i + window], q, args.target_cov).mean() for i, q in enumerate(int_q)]\n",
    "            int_regret = int_losses.values - np.asarray(opts)\n",
    "            int_miscov = np.abs(args.target_cov - int_cov)\n",
    "\n",
    "            # Do the plotting\n",
    "            color = \"C\" + str(i + (i > 0) if m is not SAOCP else 1)\n",
    "            label2err[label].append(f\"{np.max(int_miscov):.2f}\")\n",
    "            ax1.plot(range(len(int_cov)), int_cov, zorder=i, label=label, color=color)\n",
    "            ax2.plot(range(len(int_w)), gaussian_filter1d(int_w, sigma=2), zorder=i, label=label, color=color)\n",
    "            if min(int_cov) < args.target_cov - 0.2:\n",
    "                ax1.set_ylim(args.target_cov - 0.2, 1.02)\n",
    "            #\n",
    "            # f\"Avg Miscov: {np.mean(int_miscov):.3f}, \"\n",
    "            # f\"Avg Regret: {np.mean(int_regret):.4f}, \"\n",
    "            print(\n",
    "                f\"{name:15}: \"\n",
    "                f\"Avg Cov: {np.mean(coverages[name]):.3f}, \"\n",
    "                f\"Avg Width: {np.mean(widths[name]):.1f}, \"\n",
    "                f\"LCEk: {np.max(int_miscov):.2f},\"\n",
    "                f\"Runtime:\"\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    labels = []\n",
    "    lines = axs[0, 0].get_lines()\n",
    "    for line in lines:\n",
    "        label = line.get_label()\n",
    "        if label in label2err:\n",
    "            label = f\"{label}: $\\\\mathrm{{LCE}}_k = ({','.join(label2err[label])})$\"\n",
    "        labels.append(label)\n",
    "    #ncols = math.ceil(len(lines) / 2) if FACI_S in methods else len(lines)\n",
    "    ncols = 4\n",
    "    fig.subplots_adjust(top=0.92 if ncols == len(lines) else 0.88)\n",
    "    fig.legend(lines, labels, loc=\"upper center\", ncols=ncols, fontsize=10, columnspacing=1.5)\n",
    "    figdir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"figures\")\n",
    "    os.makedirs(figdir, exist_ok=True)\n",
    "    fig.savefig(os.path.join(figdir, f\"{args.dataset}_{round(D/D_old,3)}D.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
