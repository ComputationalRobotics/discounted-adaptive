{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbombara/Documents/ComputationalRoboticsCode/discounted-adaptive/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Copyright (c) 2023 salesforce.com, inc.\n",
    "# All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/Apache-2.0\n",
    "#\n",
    "\"\"\"\n",
    "File for running all computer vision experiments.\n",
    "\"\"\"\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "from re import sub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, LBFGS, SGD\n",
    "import tqdm\n",
    "\n",
    "# Our new algorithm\n",
    "from online_conformal.magnitude_learner import MagnitudeLearner\n",
    "from online_conformal.mag_learner_undiscounted import MagLearnUndiscounted\n",
    "from online_conformal.ogd_simple import SimpleOGD\n",
    "\n",
    "# From previous work\n",
    "from online_conformal.saocp import SAOCP\n",
    "from online_conformal.faci import FACI, FACI_S\n",
    "from online_conformal.nex_conformal import NExConformal\n",
    "from online_conformal.ogd import ScaleFreeOGD\n",
    "from online_conformal.split_conformal import SplitConformal\n",
    "from online_conformal.utils import pinball_loss\n",
    "from cv_utils import create_model, data_loader\n",
    "from cv_utils import ImageNet, TinyImageNet, CIFAR10, CIFAR100, ImageNetC, TinyImageNetC, CIFAR10C, CIFAR100C\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"vision.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args_local_rank =  -1\n",
      "args.devices =  cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "argparse.Namespace"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corruptions = [\n",
    "    None,\n",
    "    \"brightness\",\n",
    "    \"contrast\",\n",
    "    \"defocus_blur\",\n",
    "    \"elastic_transform\",\n",
    "    \"fog\",\n",
    "    \"frost\",\n",
    "    \"gaussian_noise\",\n",
    "    \"glass_blur\",\n",
    "    \"impulse_noise\",\n",
    "    \"jpeg_compression\",\n",
    "    \"motion_blur\",\n",
    "    \"pixelate\",\n",
    "    \"shot_noise\",\n",
    "    \"snow\",\n",
    "    \"zoom_blur\",\n",
    "]\n",
    "\n",
    "def parse_args():\n",
    "    args = argparse.Namespace\n",
    "    args.dataset = \"TinyImageNet\"\n",
    "    args.model = \"resnet50\"\n",
    "    args.lr = 1e-3\n",
    "    args.batch_size = 256\n",
    "    args.n_epochs = 150 #default = 150\n",
    "    args.patience = 10\n",
    "    #args.ignore_checkpoint = \"store_true\"\n",
    "    args.target_cov = 90\n",
    "    args.ignore_checkpoint = True\n",
    "\n",
    "    assert 50 < args.target_cov < 100\n",
    "    args.target_cov = args.target_cov / 100\n",
    "\n",
    "    # Set up distributed training if desired, and set the device\n",
    "    args.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    print(\"args_local_rank = \", args.local_rank)\n",
    "    if args.local_rank == -1:\n",
    "        if torch.cuda.is_available():\n",
    "            args.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            args.device = torch.device(\"cpu\")\n",
    "        args.world_size = 1\n",
    "    else:\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "        args.device = torch.device(args.local_rank)\n",
    "        args.world_size = dist.get_world_size()\n",
    "    print(\"args.devices = \", args.device)\n",
    "        \n",
    "    return args\n",
    "\n",
    "parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_dataset(dataset, split):\n",
    "    if dataset == \"ImageNet\":\n",
    "        return ImageNet(split)\n",
    "    elif dataset == \"TinyImageNet\":\n",
    "        return TinyImageNet(split)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        return CIFAR10(split)\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        return CIFAR100(split)\n",
    "    raise ValueError(f\"Dataset {dataset} is not supported.\")\n",
    "\n",
    "\n",
    "def get_model_file(args):\n",
    "    rootdir = os.path.dirname(os.path.abspath(__file__))\n",
    "    return os.path.join(rootdir, \"cv_models\", args.dataset, args.model, \"model.pt\")\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    if args.dataset != \"ImageNet\":\n",
    "        return torch.load(get_model_file(args), map_location=args.device)\n",
    "    return create_model(dataset=ImageNet(\"valid\"), model_name=args.model, device=args.device)\n",
    "\n",
    "\n",
    "def get_results_file(args, corruption, severity):\n",
    "    rootdir = os.path.dirname(os.path.abspath(__file__))\n",
    "    return os.path.join(rootdir, \"cv_logits\", args.dataset, args.model, f\"{corruption}_{severity}.pt\")\n",
    "\n",
    "def get_temp_file(args):\n",
    "    return os.path.join(os.path.dirname(get_results_file(args, None, 0)), \"temp.txt\")\n",
    "\n",
    "\n",
    "def finished(args):\n",
    "    for corruption in corruptions:\n",
    "        for severity in [0] if corruption is None else [1, 2, 3, 4, 5]:\n",
    "            fname = get_results_file(args, corruption, severity)\n",
    "            if not os.path.isfile(fname):\n",
    "                return False\n",
    "    return os.path.isfile(get_temp_file(args))\n",
    "\n",
    "def raps_params(dataset):\n",
    "    if dataset == \"CIFAR10\":\n",
    "        lmbda, k_reg, n_class = 0.1, 1, 10\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        lmbda, k_reg, n_class = 0.02, 5, 100\n",
    "    elif dataset == \"TinyImageNet\":\n",
    "        lmbda, k_reg, n_class = 0.01, 20, 200\n",
    "    elif dataset == \"ImageNet\":\n",
    "        lmbda, k_reg, n_class = 0.01, 10, 1000\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset {dataset}\")\n",
    "    return lmbda, k_reg, n_class\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Get train/valid data\n",
    "    print(\"Getting training and validation data\")\n",
    "    train_data = get_base_dataset(args.dataset, \"train\")\n",
    "    valid_data = get_base_dataset(args.dataset, \"valid\")\n",
    "\n",
    "    # Load model checkpoint one has been saved. Otherwise, initialize everything from scratch.\n",
    "    print(\"Load model checkpoint one has been saved. Otherwise, initialize everything from scratch.\")\n",
    "    model_file = get_model_file(args)\n",
    "    ckpt_name = os.path.join(os.path.dirname(model_file), \"checkpoint.pt\")\n",
    "    if os.path.isfile(ckpt_name) and not args.ignore_checkpoint:\n",
    "        model, opt, epoch, best_epoch, best_valid_acc = torch.load(ckpt_name, map_location=args.device)\n",
    "    else:\n",
    "        # create save directory if needed\n",
    "        print(\"Create a save directory if needed\")\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            os.makedirs(os.path.dirname(ckpt_name), exist_ok=True)\n",
    "        model = create_model(dataset=train_data, model_name=args.model, device=args.device)\n",
    "        if \"ImageNet\" in args.dataset:\n",
    "            opt = SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        else:\n",
    "            opt = Adam(model.parameters(), lr=args.lr)\n",
    "        epoch, best_epoch, best_valid_acc = 0, 0, 0.0\n",
    "\n",
    "    # Set up distributed data parallel if applicable\n",
    "    print(\"Set up distributed data parallel if applicable\")\n",
    "    writer = args.local_rank in [-1, 0]\n",
    "    if args.local_rank != -1:\n",
    "        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.device])\n",
    "\n",
    "    for epoch in range(epoch, args.n_epochs):\n",
    "        # Check early stopping condition\n",
    "        print(\"Check early stopping condition\")\n",
    "        if args.patience and epoch - best_epoch > args.patience:\n",
    "            break\n",
    "\n",
    "        # Main training loop\n",
    "        print(\"Main training loop\")\n",
    "        train_loader = data_loader(dataset=train_data, batch_size=args.batch_size // args.world_size, epoch=epoch)\n",
    "        for x, y in tqdm.tqdm(train_loader, desc=f\"Train epoch {epoch+1:2}/{args.n_epochs}\", disable=not writer):\n",
    "            opt.zero_grad()\n",
    "            pred = model(x.to(device=args.device))\n",
    "            loss = F.cross_entropy(pred, y.to(device=args.device))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # Anneal learning rate by a factor of 10 every 7 epochs\n",
    "        print(\"Anneal learning rate by a factor of 10 every 7 epochs\")\n",
    "        if (epoch + 1) % 7 == 0:\n",
    "            for g in opt.param_groups:\n",
    "                g[\"lr\"] *= 0.1\n",
    "\n",
    "        # Obtain accuracy on the validation dataset\n",
    "        print(\"Obtain accuracy on the validation dataset\")\n",
    "        valid_acc = torch.zeros(2, device=args.device)\n",
    "        valid_loader = data_loader(valid_data, batch_size=args.batch_size, epoch=epoch)\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm.tqdm(valid_loader, desc=f\"Valid epoch {epoch + 1:2}/{args.n_epochs}\", disable=True):\n",
    "                pred = model(x.to(device=args.device))\n",
    "                valid_acc[0] += x.shape[0]\n",
    "                valid_acc[1] += (pred.argmax(dim=-1) == y.to(device=args.device)).sum().item()\n",
    "\n",
    "        # Reduce results from all parallel processes\n",
    "        print(\"Reduce results from all parallel processes\")\n",
    "        if args.local_rank != -1:\n",
    "            dist.all_reduce(valid_acc)\n",
    "        valid_acc = (valid_acc[1] / valid_acc[0]).item()\n",
    "\n",
    "        # Save checkpoint & update best saved model\n",
    "        print(\"Save checkpoints and update best saved model\")\n",
    "        if writer:\n",
    "            print(f\"Epoch {epoch + 1:2} valid acc: {valid_acc:.5f}\")\n",
    "            model_to_save = model.module if args.local_rank != -1 else model\n",
    "            if valid_acc > best_valid_acc:\n",
    "                best_epoch = epoch\n",
    "                best_valid_acc = valid_acc\n",
    "                torch.save(model_to_save, model_file)\n",
    "            torch.save([model_to_save, opt, epoch + 1, best_epoch, best_valid_acc], ckpt_name)\n",
    "\n",
    "        # Synchronize before starting next epoch\n",
    "        print(\"Synchronize before starting next epoch\")\n",
    "        if args.local_rank != -1:\n",
    "            dist.barrier()\n",
    "\n",
    "\n",
    "def temperature_scaling(args):\n",
    "    temp = nn.Parameter(torch.tensor(1.0, device=args.device))\n",
    "    opt = LBFGS([temp], lr=0.01, max_iter=500)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_epochs = 10\n",
    "    valid_data = get_base_dataset(args.dataset, \"valid\")\n",
    "    model = get_model(args)\n",
    "    for epoch in range(n_epochs):\n",
    "        valid_loader = data_loader(valid_data, batch_size=args.batch_size, epoch=epoch)\n",
    "        for x, y in tqdm.tqdm(valid_loader, desc=f\"Calibration epoch {epoch + 1:2}/{n_epochs}\", disable=False):\n",
    "            with torch.no_grad():\n",
    "                logits = model(x.to(device=args.device))\n",
    "\n",
    "            def eval():\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(logits / temp, y.to(device=args.device))\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            opt.step(eval)\n",
    "\n",
    "    return temp.item()\n",
    "\n",
    "\n",
    "def get_logits(args):\n",
    "    if args.dataset == \"CIFAR10\":\n",
    "        dataset_cls = CIFAR10C\n",
    "    elif args.dataset == \"CIFAR100\":\n",
    "        dataset_cls = CIFAR100C\n",
    "    elif args.dataset == \"TinyImageNet\":\n",
    "        dataset_cls = TinyImageNetC\n",
    "    elif args.dataset == \"ImageNet\":\n",
    "        dataset_cls = ImageNetC\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {args.dataset} is not supported.\")\n",
    "    model = None\n",
    "    print(\"Dataset: \", dataset_cls)\n",
    "\n",
    "    print(\"Applying corruptions: \", corruptions)\n",
    "    for corruption in tqdm.tqdm(corruptions, desc=\"Corruptions\", position=1):\n",
    "        print(\"Corruption: \", corruption)\n",
    "        severities = [0] if corruption is None else [1, 2, 3, 4, 5]\n",
    "        print(\"Applying various severity levels: \", severities)\n",
    "        for severity in tqdm.tqdm(severities, desc=\"Severity Levels\", position=2, leave=False):\n",
    "            print(\"Severity: \", severity)\n",
    "            fname = get_results_file(args, corruption, severity)\n",
    "            if os.path.isfile(fname) and not args.ignore_checkpoint:\n",
    "                continue\n",
    "            os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "            if model is None:\n",
    "                model = get_model(args)\n",
    "\n",
    "            # Save the model's logits & labels for the whole dataset\n",
    "            print(\"Save the model's logits & labels for the whole dataset\")\n",
    "            logits, labels = [], []\n",
    "            dataset = dataset_cls(corruption=corruption, severity=severity)\n",
    "            loader = data_loader(dataset, batch_size=args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    logits.append(model(x.to(device=args.device)).cpu())\n",
    "                    labels.append(y.cpu())\n",
    "            torch.save([torch.cat(logits), torch.cat(labels)], fname)\n",
    "\n",
    "\n",
    "def t_to_sev(t, window, run_length=500, schedule=None):\n",
    "    if t < window or schedule in [None, \"None\", \"none\"]:\n",
    "        return 0\n",
    "    t_base = t - window // 2\n",
    "    if schedule == \"gradual\":\n",
    "        k = (t_base // run_length) % 10\n",
    "        return k if k <= 5 else 10 - k\n",
    "    if schedule == \"random_sudden\":\n",
    "        #return np.random.randint(0, 5) * ((t_base // run_length) % 2)\n",
    "        return np.clip(np.random.randint(0, 10) * ((t_base // run_length) % 2),0,5)\n",
    "    if schedule == \"random_gradual\":\n",
    "        k = (((t_base* abs(np.random.uniform(1,1.5))) // run_length) % 10 ) \n",
    "        return (k if k <= 5 else 10 - k) * np.random.randint(1,2) \n",
    "    return 5 * ((t_base // run_length) % 2) # default: sudden schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model, save its logits on all the corrupted test datasets, and do temperature scaling\n",
      "args_local_rank =  -1\n",
      "args.devices =  cpu\n",
      "Getting temp file...\n",
      "Done\n",
      "Load the saved logits\n",
      "Initialize conformal prediction methods, along with accumulators for results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 8924.73it/s]\n",
      "/Users/davidbombara/Documents/ComputationalRoboticsCode/discounted-adaptive/env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/davidbombara/Documents/ComputationalRoboticsCode/discounted-adaptive/env/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/davidbombara/Documents/ComputationalRoboticsCode/discounted-adaptive/env/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/davidbombara/Documents/ComputationalRoboticsCode/discounted-adaptive/env/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/davidbombara/Documents/ComputationalRoboticsCode/discounted-adaptive/env/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): nan, Std. Runtime (n=100): nan, Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9180.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.811537, Std. Runtime (n=100): 0.000000, Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9568.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.788240, Std. Runtime (n=100): 0.023297, Iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9604.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.770218, Std. Runtime (n=100): 0.031803, Iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9957.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.760538, Std. Runtime (n=100): 0.032244, Iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9986.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.749512, Std. Runtime (n=100): 0.036305, Iteration: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9635.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.741828, Std. Runtime (n=100): 0.037331, Iteration: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7012/7012 [00:00<00:00, 9831.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift: sudden\n",
      "SimpleOGD      : Cov: 0.899, Avg Width: 125.0, Avg Miscov: 0.041, Avg Regret: 0.0031, Avg. Runtime (n=100): 0.739961, Std. Runtime (n=100): 0.034863, Iteration: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1944/7012 [00:00<00:00, 9065.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(order, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mwarmup), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(order)):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Get saved results for the desired severity\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     sev \u001b[38;5;241m=\u001b[39m t_to_sev(t, window\u001b[38;5;241m=\u001b[39mwindow, schedule \u001b[38;5;241m=\u001b[39m shift)\n\u001b[0;32m---> 63\u001b[0m     probs, label \u001b[38;5;241m=\u001b[39m sev2results[sev][state\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sev2results[sev]))][i]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Convert probability to APS score\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     i_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflip(np\u001b[38;5;241m.\u001b[39margsort(probs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model, save its logits on all the corrupted test datasets, and do temperature scaling\n",
    "print(\"Train the model, save its logits on all the corrupted test datasets, and do temperature scaling\")\n",
    "args = parse_args()\n",
    "# if args.dataset != \"ImageNet\":\n",
    "if not finished(args) and args.dataset != \"ImageNet\":\n",
    "    print(\"Training models\")\n",
    "    train(args)\n",
    "    print(\"Finished training\")\n",
    "if args.local_rank in [-1, 0]:\n",
    "    print(\"Getting temp file...\")\n",
    "    temp_file = get_temp_file(args)\n",
    "    print(\"Done\")\n",
    "    if not finished(args):\n",
    "        print(\"get_logits(args)\")\n",
    "        get_logits(args)\n",
    "        print(\"...Done\")\n",
    "        temp = temperature_scaling(args)\n",
    "        with open(temp_file, \"w\") as f:\n",
    "            f.write(str(temp))\n",
    "\n",
    "    # Load the saved logits\n",
    "    print(\"Load the saved logits\")\n",
    "    with open(temp_file) as f:\n",
    "        temp = float(f.readline())\n",
    "    n_data = None\n",
    "    sev2results = defaultdict(list)\n",
    "    for corruption in corruptions:\n",
    "        severities = [0] if corruption is None else [1, 2, 3, 4, 5]\n",
    "        for severity in severities:\n",
    "            try:\n",
    "                logits, labels = torch.load(get_results_file(args, corruption, severity))\n",
    "            except:\n",
    "                continue\n",
    "            sev2results[severity].append(list(zip(F.softmax(logits / temp, dim=-1).numpy(), labels.numpy())))\n",
    "            n_data = len(labels) if n_data is None else min(n_data, len(labels))\n",
    "\n",
    "    # Initialize conformal prediction methods, along with accumulators for results\n",
    "    print(\"Initialize conformal prediction methods, along with accumulators for results\")\n",
    "    lmbda, k_reg, n_class = raps_params(args.dataset)\n",
    "    D_old = 1 + lmbda * np.sqrt(n_class - k_reg)\n",
    "    D = 1*D_old\n",
    "    #methods = [SplitConformal, NExConformal, FACI, ScaleFreeOGD, SimpleOGD, FACI_S, SAOCP, MagnitudeLearner, MagLearnUndiscounted]\n",
    "    methods = [SimpleOGD]\n",
    "    label2err = defaultdict(list)\n",
    "    h = 5 + 0.5 * (len(methods) > 5)\n",
    "    np.random.seed(0)\n",
    "    elasped_time = np.zeros(100,)\n",
    "    for jj in range(100):\n",
    "        for i_shift, shift in enumerate([\"sudden\"]):\n",
    "            sevs, s_opts, w_opts = [], [], []\n",
    "            # warmup, window, run_length = 1000, 100, 500 # original code\n",
    "            warmup, window, run_length = 1000, 25, 1000 # our code\n",
    "            state = np.random.RandomState(0)\n",
    "            order = state.permutation(n_data)[: 6 * run_length + window // 2 + warmup]\n",
    "            coverages, s_hats, widths = [{m.__name__: [] for m in methods} for _ in range(3)]\n",
    "            predictors = [m(None, None, max_scale = D, lifetime = 32, coverage = args.target_cov) for m in methods]\n",
    "            t_vec = np.zeros(len(order))\n",
    "\n",
    "            start_time = time.time()\n",
    "            for t, i in tqdm.tqdm(enumerate(order, start=-warmup), total=len(order)):\n",
    "                # Get saved results for the desired severity\n",
    "                sev = t_to_sev(t, window=window, schedule = shift)\n",
    "                probs, label = sev2results[sev][state.randint(0, len(sev2results[sev]))][i]\n",
    "\n",
    "                # Convert probability to APS score\n",
    "                i_sort = np.flip(np.argsort(probs))\n",
    "                p_sort_cumsum = np.cumsum(probs[i_sort]) - state.rand() * probs[i_sort]\n",
    "                s_sort_cumsum = p_sort_cumsum + lmbda * np.sqrt(np.cumsum([i > k_reg for i in range(n_class)]))\n",
    "                w_opt = np.argsort(i_sort)[label] + 1\n",
    "                s_opt = s_sort_cumsum[w_opt - 1]\n",
    "                if t >= 0:\n",
    "                    sevs.append(sev)\n",
    "                    s_opts.append(s_opt)\n",
    "                    w_opts.append(w_opt)\n",
    "                    t_vec[t] = t\n",
    "\n",
    "                # Update all the conformal predictors\n",
    "                for predictor in predictors:\n",
    "                    name = type(predictor).__name__\n",
    "                    if t >= 0:\n",
    "                        _, s_hat = predictor.predict(horizon=1)\n",
    "                        w = np.sum(s_sort_cumsum <= s_hat)\n",
    "                        s_hats[name].append(s_hat)\n",
    "                        widths[name].append(w)\n",
    "                        coverages[name].append(w >= w_opt)\n",
    "                    predictor.update(ground_truth=pd.Series([s_opt]), forecast=pd.Series([0]), horizon=1)\n",
    "            elasped_time[jj] = time.time() - start_time\n",
    "            # Perform evaluation & produce a pretty graph\n",
    "            plot_loss = False\n",
    "\n",
    "            s_opts = np.asarray(s_opts)\n",
    "            int_q = pd.Series(s_opts).rolling(window).quantile(args.target_cov).dropna()\n",
    "            print(f\"Distribution shift: {shift}\")\n",
    "            for i, m in enumerate(methods):\n",
    "            # Compute various summary statistics\n",
    "                name = m.__name__ # name of the method (OGD, SAOCP, etc.)\n",
    "                label = sub(\"Split\", \"S\", sub(\"Conformal\", \"CP\", sub(\"ScaleFree\", \"SF-\", sub(\"_\", \"-\", name))))\n",
    "                s_hat = np.asarray(s_hats[name])\n",
    "                int_cov = gaussian_filter1d(pd.Series(coverages[name]).rolling(window).mean().dropna(), sigma=3)\n",
    "                int_w = pd.Series(s_hats[name] if plot_loss else widths[name]).rolling(window).mean().dropna()\n",
    "                int_losses = pd.Series(pinball_loss(s_opts, s_hat, args.target_cov)).rolling(window).mean().dropna()\n",
    "                opts = [pinball_loss(s_opts[i : i + window], q, args.target_cov).mean() for i, q in enumerate(int_q)]\n",
    "                int_regret = int_losses.values - np.asarray(opts)\n",
    "                int_miscov = np.abs(args.target_cov - int_cov)\n",
    "\n",
    "                # Do the plotting\n",
    "                label2err[label].append(f\"{np.max(int_miscov):.2f}\")\n",
    "\n",
    "                print(\n",
    "                    f\"{name:15}: \"\n",
    "                    f\"Cov: {np.mean(coverages[name]):.3f}, \"\n",
    "                    f\"Avg Width: {np.mean(widths[name]):.1f}, \"\n",
    "                    f\"Avg Miscov: {np.mean(int_miscov):.3f}, \"\n",
    "                    f\"Avg Regret: {np.mean(int_regret):.4f}, \"\n",
    "                    f\"Avg. Runtime (n=100): {np.mean(elasped_time[0:jj]):.6f}, \"\n",
    "                    f\"Std. Runtime (n=100): {np.std(elasped_time[0:jj]):.6f}, \"\n",
    "                    f\"Iteration: {jj:.0f}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
